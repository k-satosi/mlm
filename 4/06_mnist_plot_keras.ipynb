{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.optimizers import SGD\n",
    "from keras import backend as K\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "\n",
    "'''\n",
    "データの生成\n",
    "'''\n",
    "mnist = datasets.fetch_mldata('MNIST original', data_home='.')\n",
    "\n",
    "n = len(mnist.data)\n",
    "N = 30000  # MNISTの一部を使う\n",
    "N_train = 20000\n",
    "N_validation = 4000\n",
    "indices = np.random.permutation(range(n))[:N]  # ランダムにN枚を選択\n",
    "\n",
    "X = mnist.data[indices]\n",
    "y = mnist.target[indices]\n",
    "Y = np.eye(10)[y.astype(int)]\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = \\\n",
    "    train_test_split(X, Y, test_size=N_validation)\n",
    "X_train, X_validation, Y_train, Y_validation = \\\n",
    "    train_test_split(X_train, Y_train, test_size=N_validation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "モデル設定\n",
    "'''\n",
    "n_in = len(X[0])  # 784\n",
    "n_hiddens = [200, 200, 200]\n",
    "n_out = len(Y[0])  # 10\n",
    "p_keep = 0.5\n",
    "activation = 'relu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "    # 標準偏差0.01の切断正規分布に従う乱数を返す\n",
    "    return K.truncated_normal(shape, stddev=0.01)\n",
    "    # return np.random.normal(scale=0.01, size=shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "for i, input_dim in enumerate(([n_in] + n_hiddens)[:-1]):\n",
    "    model.add(Dense(n_hiddens[i], input_dim=input_dim,\n",
    "                    kernel_initializer=weight_variable))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(Dropout(p_keep))\n",
    "\n",
    "model.add(Dense(n_out, kernel_initializer=weight_variable))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=SGD(lr=0.01),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22000 samples, validate on 4000 samples\n",
      "Epoch 1/50\n",
      "22000/22000 [==============================] - 2s 106us/step - loss: 2.1686 - acc: 0.1866 - val_loss: 1.5564 - val_acc: 0.5437\n",
      "Epoch 2/50\n",
      "22000/22000 [==============================] - 2s 78us/step - loss: 1.1072 - acc: 0.6232 - val_loss: 0.5698 - val_acc: 0.8400\n",
      "Epoch 3/50\n",
      "22000/22000 [==============================] - 2s 75us/step - loss: 0.6589 - acc: 0.7953 - val_loss: 0.3891 - val_acc: 0.8953\n",
      "Epoch 4/50\n",
      "22000/22000 [==============================] - 2s 77us/step - loss: 0.5068 - acc: 0.8479 - val_loss: 0.3094 - val_acc: 0.9130\n",
      "Epoch 5/50\n",
      "22000/22000 [==============================] - 2s 79us/step - loss: 0.4224 - acc: 0.8780 - val_loss: 0.2588 - val_acc: 0.9282\n",
      "Epoch 6/50\n",
      "22000/22000 [==============================] - 2s 76us/step - loss: 0.3643 - acc: 0.8964 - val_loss: 0.2395 - val_acc: 0.9345\n",
      "Epoch 7/50\n",
      "22000/22000 [==============================] - 2s 77us/step - loss: 0.3290 - acc: 0.9054 - val_loss: 0.2229 - val_acc: 0.9370\n",
      "Epoch 8/50\n",
      "22000/22000 [==============================] - 2s 76us/step - loss: 0.2951 - acc: 0.9157 - val_loss: 0.2037 - val_acc: 0.9422\n",
      "Epoch 9/50\n",
      "22000/22000 [==============================] - 2s 78us/step - loss: 0.2772 - acc: 0.9203 - val_loss: 0.1917 - val_acc: 0.9463\n",
      "Epoch 10/50\n",
      "22000/22000 [==============================] - 2s 102us/step - loss: 0.2549 - acc: 0.9265 - val_loss: 0.1802 - val_acc: 0.9490\n",
      "Epoch 11/50\n",
      "22000/22000 [==============================] - 2s 93us/step - loss: 0.2383 - acc: 0.9331 - val_loss: 0.1800 - val_acc: 0.9497\n",
      "Epoch 12/50\n",
      "22000/22000 [==============================] - 2s 104us/step - loss: 0.2289 - acc: 0.9325 - val_loss: 0.1669 - val_acc: 0.9507\n",
      "Epoch 13/50\n",
      "22000/22000 [==============================] - 2s 77us/step - loss: 0.2176 - acc: 0.9395 - val_loss: 0.1681 - val_acc: 0.9523\n",
      "Epoch 14/50\n",
      "22000/22000 [==============================] - 2s 79us/step - loss: 0.2035 - acc: 0.9419 - val_loss: 0.1620 - val_acc: 0.9540\n",
      "Epoch 15/50\n",
      "22000/22000 [==============================] - 2s 77us/step - loss: 0.1948 - acc: 0.9438 - val_loss: 0.1506 - val_acc: 0.9585\n",
      "Epoch 16/50\n",
      "22000/22000 [==============================] - 2s 79us/step - loss: 0.1864 - acc: 0.9464 - val_loss: 0.1585 - val_acc: 0.9587\n",
      "Epoch 17/50\n",
      "22000/22000 [==============================] - 2s 77us/step - loss: 0.1741 - acc: 0.9499 - val_loss: 0.1493 - val_acc: 0.9597\n",
      "Epoch 18/50\n",
      "22000/22000 [==============================] - 2s 83us/step - loss: 0.1703 - acc: 0.9515 - val_loss: 0.1495 - val_acc: 0.9600\n",
      "Epoch 19/50\n",
      "22000/22000 [==============================] - 2s 79us/step - loss: 0.1685 - acc: 0.9522 - val_loss: 0.1528 - val_acc: 0.9577\n",
      "Epoch 20/50\n",
      "22000/22000 [==============================] - 2s 81us/step - loss: 0.1603 - acc: 0.9530 - val_loss: 0.1458 - val_acc: 0.9588\n",
      "Epoch 21/50\n",
      "22000/22000 [==============================] - 2s 78us/step - loss: 0.1535 - acc: 0.9555 - val_loss: 0.1414 - val_acc: 0.9627\n",
      "Epoch 22/50\n",
      "22000/22000 [==============================] - 2s 78us/step - loss: 0.1535 - acc: 0.9545 - val_loss: 0.1433 - val_acc: 0.9618\n",
      "Epoch 23/50\n",
      "22000/22000 [==============================] - 2s 76us/step - loss: 0.1484 - acc: 0.9574 - val_loss: 0.1399 - val_acc: 0.9640\n",
      "Epoch 24/50\n",
      "22000/22000 [==============================] - 2s 75us/step - loss: 0.1445 - acc: 0.9577 - val_loss: 0.1374 - val_acc: 0.9642\n",
      "Epoch 25/50\n",
      "22000/22000 [==============================] - 2s 76us/step - loss: 0.1386 - acc: 0.9591 - val_loss: 0.1328 - val_acc: 0.9652\n",
      "Epoch 26/50\n",
      "22000/22000 [==============================] - 2s 77us/step - loss: 0.1361 - acc: 0.9610 - val_loss: 0.1352 - val_acc: 0.9620\n",
      "Epoch 27/50\n",
      "22000/22000 [==============================] - 2s 76us/step - loss: 0.1308 - acc: 0.9607 - val_loss: 0.1410 - val_acc: 0.9648\n",
      "Epoch 28/50\n",
      "22000/22000 [==============================] - 2s 77us/step - loss: 0.1244 - acc: 0.9646 - val_loss: 0.1382 - val_acc: 0.9648\n",
      "Epoch 29/50\n",
      "22000/22000 [==============================] - 2s 76us/step - loss: 0.1286 - acc: 0.9625 - val_loss: 0.1349 - val_acc: 0.9640\n",
      "Epoch 30/50\n",
      "22000/22000 [==============================] - 2s 76us/step - loss: 0.1212 - acc: 0.9640 - val_loss: 0.1386 - val_acc: 0.9635\n",
      "Epoch 31/50\n",
      "22000/22000 [==============================] - 2s 78us/step - loss: 0.1230 - acc: 0.9649 - val_loss: 0.1352 - val_acc: 0.9638\n",
      "Epoch 32/50\n",
      "22000/22000 [==============================] - 2s 77us/step - loss: 0.1122 - acc: 0.9671 - val_loss: 0.1338 - val_acc: 0.9660\n",
      "Epoch 33/50\n",
      "22000/22000 [==============================] - 2s 76us/step - loss: 0.1124 - acc: 0.9664 - val_loss: 0.1358 - val_acc: 0.9655\n",
      "Epoch 34/50\n",
      "22000/22000 [==============================] - 2s 77us/step - loss: 0.1094 - acc: 0.9669 - val_loss: 0.1380 - val_acc: 0.9650\n",
      "Epoch 35/50\n",
      "22000/22000 [==============================] - 2s 76us/step - loss: 0.1031 - acc: 0.9690 - val_loss: 0.1374 - val_acc: 0.9642\n",
      "Epoch 36/50\n",
      "22000/22000 [==============================] - 2s 76us/step - loss: 0.1025 - acc: 0.9706 - val_loss: 0.1402 - val_acc: 0.9655\n",
      "Epoch 37/50\n",
      "22000/22000 [==============================] - 2s 77us/step - loss: 0.0990 - acc: 0.9703 - val_loss: 0.1316 - val_acc: 0.9670\n",
      "Epoch 38/50\n",
      "22000/22000 [==============================] - 2s 76us/step - loss: 0.1067 - acc: 0.9690 - val_loss: 0.1319 - val_acc: 0.9658\n",
      "Epoch 39/50\n",
      "22000/22000 [==============================] - 2s 77us/step - loss: 0.1028 - acc: 0.9691 - val_loss: 0.1353 - val_acc: 0.9670\n",
      "Epoch 40/50\n",
      "22000/22000 [==============================] - 2s 76us/step - loss: 0.0975 - acc: 0.9701 - val_loss: 0.1431 - val_acc: 0.9660\n",
      "Epoch 41/50\n",
      "22000/22000 [==============================] - 2s 76us/step - loss: 0.0945 - acc: 0.9719 - val_loss: 0.1380 - val_acc: 0.9658\n",
      "Epoch 42/50\n",
      "22000/22000 [==============================] - 2s 77us/step - loss: 0.0891 - acc: 0.9731 - val_loss: 0.1320 - val_acc: 0.9688\n",
      "Epoch 43/50\n",
      "22000/22000 [==============================] - 2s 78us/step - loss: 0.0995 - acc: 0.9698 - val_loss: 0.1267 - val_acc: 0.9670\n",
      "Epoch 44/50\n",
      "22000/22000 [==============================] - 2s 77us/step - loss: 0.0917 - acc: 0.9728 - val_loss: 0.1294 - val_acc: 0.9685\n",
      "Epoch 45/50\n",
      "22000/22000 [==============================] - 2s 76us/step - loss: 0.0937 - acc: 0.9711 - val_loss: 0.1350 - val_acc: 0.9685\n",
      "Epoch 46/50\n",
      "22000/22000 [==============================] - 2s 76us/step - loss: 0.0892 - acc: 0.9731 - val_loss: 0.1327 - val_acc: 0.9683\n",
      "Epoch 47/50\n",
      "22000/22000 [==============================] - 2s 76us/step - loss: 0.0843 - acc: 0.9747 - val_loss: 0.1357 - val_acc: 0.9665\n",
      "Epoch 48/50\n",
      "22000/22000 [==============================] - 2s 77us/step - loss: 0.0900 - acc: 0.9727 - val_loss: 0.1326 - val_acc: 0.9675\n",
      "Epoch 49/50\n",
      "22000/22000 [==============================] - 2s 79us/step - loss: 0.0835 - acc: 0.9755 - val_loss: 0.1378 - val_acc: 0.9658\n",
      "Epoch 50/50\n",
      "22000/22000 [==============================] - 2s 77us/step - loss: 0.0879 - acc: 0.9745 - val_loss: 0.1321 - val_acc: 0.9678\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "モデル学習\n",
    "'''\n",
    "epochs = 50\n",
    "batch_size = 200\n",
    "\n",
    "hist = model.fit(X_train, Y_train, epochs=epochs,\n",
    "                 batch_size=batch_size,\n",
    "                 validation_data=(X_validation, Y_validation))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEICAYAAACktLTqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a22fe74a8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''\n",
    "学習の進み具合を可視化\n",
    "'''\n",
    "val_acc = hist.history['val_acc']\n",
    "val_loss = hist.history['val_loss']\n",
    "\n",
    "plt.rc('font', family='serif')\n",
    "fig = plt.figure()\n",
    "plt.plot(range(epochs), val_acc, label='acc', color='black')\n",
    "plt.xlabel('epochs')\n",
    "plt.show()\n",
    "# plt.savefig('mnist_keras.eps')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000/4000 [==============================] - 0s 64us/step\n",
      "[0.11444434746610932, 0.96950000000000003]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "予測精度の評価\n",
    "'''\n",
    "loss_and_metrics = model.evaluate(X_test, Y_test)\n",
    "print(loss_and_metrics)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
